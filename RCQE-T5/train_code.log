2024-04-15 15:16:25.539819: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-04-15 15:16:25.604678: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-04-15 15:16:25.883324: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory
2024-04-15 15:16:25.883367: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory
2024-04-15 15:16:25.883371: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.
/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.2.1) or chardet (None)/charset_normalizer (2.1.0) doesn't match a supported version!
  warnings.warn(
Namespace(adam_epsilon=1e-08, add_lang_ids=False, add_task_prefix=False, beam_size=10, config_name='', data_dir='./dataset', do_eval=False, do_lower_case=False, do_test=True, do_train=True, early_stop_threshold=20, eval_batch_size=1000, eval_steps=-1, gradient_accumulation_steps=2, learning_rate=5e-05, load_model_path=None, local_rank=-1, max_grad_norm=1.0, max_source_length=512, max_steps=-1, max_target_length=5, model_name_or_path=None, model_type=None, no_cuda=False, num_train_epochs=5, output_dir='./mcc_model_code', seed=4, tokenizer_name='', train_batch_size=1000, train_steps=-1, visible_gpu='1', warm_up_ratio=0.1, weight_decay=0.0)
04/15/2024 15:16:26 - INFO - __main__ -   Namespace(adam_epsilon=1e-08, add_lang_ids=False, add_task_prefix=False, beam_size=10, config_name='', data_dir='./dataset', do_eval=False, do_lower_case=False, do_test=True, do_train=True, early_stop_threshold=20, eval_batch_size=1000, eval_steps=-1, gradient_accumulation_steps=2, learning_rate=5e-05, load_model_path=None, local_rank=-1, max_grad_norm=1.0, max_source_length=512, max_steps=-1, max_target_length=5, model_name_or_path=None, model_type=None, no_cuda=False, num_train_epochs=5, output_dir='./mcc_model_code', seed=4, tokenizer_name='', train_batch_size=1000, train_steps=-1, visible_gpu='1', warm_up_ratio=0.1, weight_decay=0.0)
Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
04/15/2024 15:16:26 - WARNING - __main__ -   Process rank: -1, device: cuda, n_gpu: 1, distributed training: False
Model created!!
04/15/2024 15:16:28 - INFO - __main__ -   Model created!!
[[{'text': 'Old Code:              if (s_logger.isDebugEnabled()) {-                s_logger.debug(String.format("Disk Offering change for the root volume is disabled during the compute offering change operation. Please check the setting %s", AllowDiskOfferingChangeDuringScaleVm.key()));             }             return;         }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' New Code:              if (s_logger.isDebugEnabled()) {+                s_logger.debug(String.format("Changing the disk offering of the root volume during the compute offering change operation is disabled. Please check the setting [%s].", AllowDiskOfferingChangeDuringScaleVm.key()));             }             return;         }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' The New Code change is', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'good'}]
04/15/2024 15:16:32 - INFO - __main__ -   [[{'text': 'Old Code:              if (s_logger.isDebugEnabled()) {-                s_logger.debug(String.format("Disk Offering change for the root volume is disabled during the compute offering change operation. Please check the setting %s", AllowDiskOfferingChangeDuringScaleVm.key()));             }             return;         }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' New Code:              if (s_logger.isDebugEnabled()) {+                s_logger.debug(String.format("Changing the disk offering of the root volume during the compute offering change operation is disabled. Please check the setting [%s].", AllowDiskOfferingChangeDuringScaleVm.key()));             }             return;         }', 'loss_ids': 0, 'shortenable_ids': 1}, {'text': ' The New Code change is', 'loss_ids': 0, 'shortenable_ids': 0}, {'text': '<mask>', 'loss_ids': 1, 'shortenable_ids': 0}], {'guid': 0, 'tgt_text': 'good'}]
torch.cuda.is_available():  True
len examples
14877
tokenizing: 0it [00:00, ?it/s]Token indices sequence length is longer than the specified maximum sequence length for this model (575 > 512). Running this sequence through the model will result in indexing errors
tokenizing: 72it [00:00, 717.96it/s]tokenizing: 153it [00:00, 769.47it/s]tokenizing: 236it [00:00, 795.28it/s]tokenizing: 319it [00:00, 805.33it/s]tokenizing: 408it [00:00, 835.26it/s]tokenizing: 496it [00:00, 848.00it/s]tokenizing: 581it [00:00, 846.47it/s]tokenizing: 668it [00:00, 851.69it/s]tokenizing: 754it [00:00, 853.97it/s]tokenizing: 851it [00:01, 889.48it/s]tokenizing: 943it [00:01, 897.82it/s]tokenizing: 1036it [00:01, 906.69it/s]tokenizing: 1127it [00:01, 900.10it/s]tokenizing: 1218it [00:01, 895.33it/s]tokenizing: 1311it [00:01, 903.60it/s]tokenizing: 1402it [00:01, 901.18it/s]tokenizing: 1494it [00:01, 904.34it/s]tokenizing: 1590it [00:01, 918.04it/s]tokenizing: 1686it [00:01, 927.29it/s]tokenizing: 1779it [00:02, 923.58it/s]tokenizing: 1872it [00:02, 919.91it/s]tokenizing: 1964it [00:02, 919.83it/s]tokenizing: 2060it [00:02, 926.29it/s]tokenizing: 2153it [00:02, 912.06it/s]tokenizing: 2247it [00:02, 917.92it/s]tokenizing: 2344it [00:02, 930.55it/s]tokenizing: 2438it [00:02, 926.11it/s]tokenizing: 2532it [00:02, 928.96it/s]tokenizing: 2627it [00:02, 933.84it/s]tokenizing: 2721it [00:03, 918.70it/s]tokenizing: 2813it [00:03, 912.96it/s]tokenizing: 2907it [00:03, 920.10it/s]tokenizing: 3005it [00:03, 936.51it/s]tokenizing: 3099it [00:03, 921.18it/s]tokenizing: 3193it [00:03, 926.49it/s]tokenizing: 3286it [00:03, 927.25it/s]tokenizing: 3379it [00:03, 919.99it/s]tokenizing: 3472it [00:03, 917.28it/s]tokenizing: 3564it [00:03, 911.07it/s]tokenizing: 3656it [00:04, 912.55it/s]tokenizing: 3749it [00:04, 915.44it/s]tokenizing: 3841it [00:04, 914.27it/s]tokenizing: 3936it [00:04, 922.88it/s]tokenizing: 4033it [00:04, 936.25it/s]tokenizing: 4128it [00:04, 934.93it/s]tokenizing: 4223it [00:04, 937.29it/s]tokenizing: 4317it [00:04, 931.07it/s]tokenizing: 4417it [00:04, 951.36it/s]tokenizing: 4513it [00:04, 944.97it/s]tokenizing: 4608it [00:05, 930.16it/s]tokenizing: 4707it [00:05, 946.06it/s]tokenizing: 4808it [00:05, 959.74it/s]tokenizing: 4905it [00:05, 948.97it/s]tokenizing: 5007it [00:05, 964.67it/s]tokenizing: 5106it [00:05, 970.43it/s]tokenizing: 5204it [00:05, 971.68it/s]tokenizing: 5302it [00:05, 969.72it/s]tokenizing: 5399it [00:05, 949.26it/s]tokenizing: 5495it [00:05, 947.30it/s]tokenizing: 5592it [00:06, 952.07it/s]tokenizing: 5688it [00:06, 954.34it/s]tokenizing: 5784it [00:06, 942.18it/s]tokenizing: 5879it [00:06, 943.55it/s]tokenizing: 5974it [00:06, 883.16it/s]tokenizing: 6077it [00:06, 920.72it/s]tokenizing: 6175it [00:06, 935.90it/s]tokenizing: 6270it [00:06, 939.75it/s]tokenizing: 6368it [00:06, 948.64it/s]tokenizing: 6468it [00:07, 961.47it/s]tokenizing: 6566it [00:07, 964.34it/s]tokenizing: 6666it [00:07, 974.53it/s]tokenizing: 6765it [00:07, 978.98it/s]tokenizing: 6863it [00:07, 975.17it/s]tokenizing: 6961it [00:07, 976.41it/s]tokenizing: 7059it [00:07, 976.92it/s]tokenizing: 7159it [00:07, 983.42it/s]tokenizing: 7258it [00:07, 955.84it/s]tokenizing: 7359it [00:07, 968.14it/s]tokenizing: 7457it [00:08, 969.91it/s]tokenizing: 7555it [00:08, 965.28it/s]tokenizing: 7652it [00:08, 966.31it/s]tokenizing: 7749it [00:08, 950.14it/s]tokenizing: 7848it [00:08, 961.71it/s]tokenizing: 7945it [00:08, 953.55it/s]tokenizing: 8045it [00:08, 967.22it/s]tokenizing: 8145it [00:08, 974.68it/s]tokenizing: 8248it [00:08, 990.66it/s]tokenizing: 8348it [00:08, 982.41it/s]tokenizing: 8447it [00:09, 964.36it/s]tokenizing: 8544it [00:09, 965.69it/s]tokenizing: 8641it [00:09, 963.84it/s]tokenizing: 8744it [00:09, 981.00it/s]tokenizing: 8843it [00:09, 965.41it/s]tokenizing: 8943it [00:09, 972.84it/s]tokenizing: 9048it [00:09, 992.90it/s]tokenizing: 9157it [00:09, 1020.59it/s]tokenizing: 9260it [00:09, 1015.90it/s]tokenizing: 9368it [00:09, 1033.39it/s]tokenizing: 9475it [00:10, 1042.01it/s]tokenizing: 9580it [00:10, 1012.70it/s]tokenizing: 9682it [00:10, 1003.23it/s]tokenizing: 9783it [00:10, 991.00it/s] tokenizing: 9885it [00:10, 996.55it/s]tokenizing: 9987it [00:10, 1001.54it/s]tokenizing: 10094it [00:10, 1020.50it/s]tokenizing: 10197it [00:10, 1003.27it/s]tokenizing: 10298it [00:10, 1000.81it/s]tokenizing: 10403it [00:11, 1013.66it/s]tokenizing: 10505it [00:11, 993.06it/s] tokenizing: 10606it [00:11, 997.55it/s]tokenizing: 10706it [00:11, 982.95it/s]tokenizing: 10805it [00:11, 975.54it/s]tokenizing: 10903it [00:11, 976.69it/s]tokenizing: 11004it [00:11, 984.77it/s]tokenizing: 11109it [00:11, 1003.88it/s]tokenizing: 11210it [00:11, 1001.42it/s]tokenizing: 11311it [00:11, 978.49it/s] tokenizing: 11409it [00:12, 976.17it/s]tokenizing: 11507it [00:12, 974.36it/s]tokenizing: 11605it [00:12, 973.86it/s]tokenizing: 11703it [00:12, 957.19it/s]tokenizing: 11799it [00:12, 954.59it/s]tokenizing: 11900it [00:12, 968.63it/s]tokenizing: 11997it [00:12, 771.11it/s]tokenizing: 12098it [00:12, 831.09it/s]tokenizing: 12204it [00:12, 891.40it/s]tokenizing: 12305it [00:13, 923.61it/s]tokenizing: 12410it [00:13, 957.70it/s]tokenizing: 12513it [00:13, 976.28it/s]tokenizing: 12616it [00:13, 989.88it/s]tokenizing: 12721it [00:13, 1005.76it/s]tokenizing: 12823it [00:13, 993.35it/s] tokenizing: 12924it [00:13, 990.96it/s]tokenizing: 13024it [00:13, 989.86it/s]tokenizing: 13124it [00:13, 990.58it/s]tokenizing: 13224it [00:13, 982.64it/s]tokenizing: 13324it [00:14, 987.42it/s]tokenizing: 13424it [00:14, 990.09it/s]tokenizing: 13529it [00:14, 1006.44it/s]tokenizing: 13633it [00:14, 1015.33it/s]tokenizing: 13735it [00:14, 1007.83it/s]tokenizing: 13836it [00:14, 1005.13it/s]tokenizing: 13938it [00:14, 1007.74it/s]tokenizing: 14040it [00:14, 1009.62it/s]tokenizing: 14141it [00:14, 1003.13it/s]tokenizing: 14242it [00:14, 995.03it/s] tokenizing: 14348it [00:15, 1013.36it/s]tokenizing: 14450it [00:15, 1008.04it/s]tokenizing: 14551it [00:15, 983.58it/s] tokenizing: 14652it [00:15, 987.98it/s]tokenizing: 14758it [00:15, 1007.00it/s]tokenizing: 14859it [00:15, 998.64it/s] tokenizing: 14877it [00:15, 953.44it/s]
/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
04/15/2024 15:16:47 - INFO - __main__ -   ***** Running training *****
  Num examples = 14877
04/15/2024 15:16:47 - INFO - __main__ -     Num examples = 14877
  Batch size = 1000
04/15/2024 15:16:47 - INFO - __main__ -     Batch size = 1000
  Num epoch = 5
04/15/2024 15:16:47 - INFO - __main__ -     Num epoch = 5
  0%|          | 0/15 [00:00<?, ?it/s]  0%|          | 0/15 [00:00<?, ?it/s]
Traceback (most recent call last):
  File "prompt_mcc_code.py", line 576, in <module>
    main(my_args)
  File "prompt_mcc_code.py", line 276, in main
    loss = model(batch)
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/openprompt/pipeline_base.py", line 449, in forward
    return self._forward(*args, **kwargs)
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/openprompt/pipeline_base.py", line 465, in _forward
    outputs = self.prompt_model(batch)
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/openprompt/pipeline_base.py", line 212, in forward
    outputs = self.plm(**input_batch, output_hidden_states=True)
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 1602, in forward
    encoder_outputs = self.encoder(
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 1035, in forward
    layer_outputs = layer_module(
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 666, in forward
    self_attention_outputs = self.layer[0](
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/openprompt/prompts/prefix_tuning_template.py", line 205, in modified_encoder_forward
    return backup_encoder_forward_functions[layer_id](*args, **kwargs)
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 572, in forward
    attention_output = self.SelfAttention(
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1190, in _call_impl
    return forward_call(*input, **kwargs)
  File "/data/anaconda3/envs/zhujie/lib/python3.8/site-packages/transformers/models/t5/modeling_t5.py", line 529, in forward
    position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 10.87 GiB (GPU 0; 23.68 GiB total capacity; 20.94 GiB already allocated; 2.28 GiB free; 20.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
